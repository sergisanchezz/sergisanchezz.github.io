<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="In recent years there have been significant advancements in the field of autonomous driving, led by improvements in perception, planning and control systems. Modern autonomous vehicles rely on complex sensor suites including cameras and LiDAR scanners to interpret their surroundings and make real-time decisions. A current major challenge lies in predicting future scenes, which is crucial to enable safe and reliable navigation.
Recent research trends in autonomous driving explore the usage of world models to perform the prediction of such future scenes. These models take in sensor inputs from a number of frames in the past and infers the scene and trajectory of the ego vehicle for the next frames. Current state-ofthe- art (SOTA) world models leverage different representations of the scene to perform this task, leading to the following pipeline categories:
" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://sergisanchezz.github.io/projects/depth-guided-world-models/" />


    <title>
        
            Towards Depth-Guided Self-Supervised World Models :: Sergi Sanchez Orvay 
        
    </title>





  <link rel="stylesheet" href="/main.min.7289daf2e88ea2c7b253a51cc5794c7cc17e7aeb7295a19d978db764a5862f25.css" integrity="sha256-cona8uiOoseyU6UcxXlMfMF&#43;eutylaGdl423ZKWGLyU=" crossorigin="anonymous">





    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Towards Depth-Guided Self-Supervised World Models">
  <meta itemprop="description" content="In recent years there have been significant advancements in the field of autonomous driving, led by improvements in perception, planning and control systems. Modern autonomous vehicles rely on complex sensor suites including cameras and LiDAR scanners to interpret their surroundings and make real-time decisions. A current major challenge lies in predicting future scenes, which is crucial to enable safe and reliable navigation.
Recent research trends in autonomous driving explore the usage of world models to perform the prediction of such future scenes. These models take in sensor inputs from a number of frames in the past and infers the scene and trajectory of the ego vehicle for the next frames. Current state-ofthe- art (SOTA) world models leverage different representations of the scene to perform this task, leading to the following pipeline categories:">
  <meta itemprop="wordCount" content="446">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Towards Depth-Guided Self-Supervised World Models">
  <meta name="twitter:description" content="In recent years there have been significant advancements in the field of autonomous driving, led by improvements in perception, planning and control systems. Modern autonomous vehicles rely on complex sensor suites including cameras and LiDAR scanners to interpret their surroundings and make real-time decisions. A current major challenge lies in predicting future scenes, which is crucial to enable safe and reliable navigation.
Recent research trends in autonomous driving explore the usage of world models to perform the prediction of such future scenes. These models take in sensor inputs from a number of frames in the past and infers the scene and trajectory of the ego vehicle for the next frames. Current state-ofthe- art (SOTA) world models leverage different representations of the scene to perform this task, leading to the following pipeline categories:">


















    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                Sergi Sanchez Orvay</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
                <nav class="menu">
    <ul class="menu__inner">
                <li><a href="/about/">About Me</a></li>
            
                <li><a href="/news/">News</a></li>
            
                <li><a href="/projects/">Projects</a></li>
            
                <li><a href="/publications/">Publications</a></li>
            
    </ul>
</nav>


                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
        </span>
    </span>
</header>


            
    <div class="post">
        <h1 class="post-title">Towards Depth-Guided Self-Supervised World Models</h1>
        
        
        <div class="post-meta" style="margin-bottom: 20px;">
            <em>Feb 2025 
            – 
            
                Jul 2025
            </em>
        </div>
        

        

        
        <h3 style="margin-top: 25px;">Contributors</h3>
        <p>
            <b>Authors:</b> Juan Tarazona Rodríguez, Ahmed Elgaradiny, Hanqiu Li Cai, Sergi Sánchez Orvay
            
                <br> <b>Supervisors:</b> Mirlan Karimov
            
        </p>
        

        <h3 style="margin-top: 30px;">Abstract</h3>
        <div class="post-content">
            <p>In recent years there have been significant advancements in the field of autonomous driving, led by improvements in perception, planning and control systems. Modern autonomous vehicles rely on complex sensor suites including cameras and LiDAR scanners to interpret their surroundings and make real-time decisions. A current major challenge lies in predicting future scenes, which is crucial to enable safe and reliable navigation.</p>
<p>Recent research trends in autonomous driving explore the usage of world models to perform the prediction of such future scenes. These models take in sensor inputs from a number of frames in the past and infers the scene and trajectory of the ego vehicle for the next frames. Current state-ofthe- art (SOTA) world models leverage different representations of the scene to perform this task, leading to the following pipeline categories:</p>
<ul>
<li>
<p>Image-based These models leverage RGB data from cameras. Their main limitation lies in the inability to fully capture the details of the 3D environment.</p>
</li>
<li>
<p>BEV-based Bird’s-Eye view (BEV) converts multimodal sensor data into a top-down view in 2D. These models suffer from difficulty capturing fine-grained 3D geometries in certain scenarios.</p>
</li>
<li>
<p>OG-based Occupancy grids (OG) discretize the environment into a voxel grid, encoding high-fidelity 3D spatial information. The main limitation lies in the large memory and computational requirements.</p>
</li>
<li>
<p>PC-based PC (Point Cloud) based models use 3D scanning data from LiDAR sensors. Despite their precision, these models are computationally intensive due to data sparsity and high input dimensionality.</p>
</li>
</ul>
<p>However, all these approaches treat geometry and appearance
separately, potentially limiting their ability to
capture the full complexity of driving environments.</p>
<p>The original project proposal aimed to develop a unified multimodal world model modelled by a spatio-temporal transformer or a diffusion model that jointly predicts both point clouds and video frames. However, after the initial literature review phase on the topic, it was concluded that such a model would be infeasible to train given the limited compute available to us and short timeframe of the course. Hence, it was decided that we would shift our focus towards improving existing self-supervised representation learning methods from RGB images by enhancing them with depth guidance, thus enabling them to encode implicit 3D information in their features.</p>
<p>As a baseline, we used I-JEPA (Image Joint Embedding Predictive Architecture), a self-supervised learning method that predicts masked regions in latent space, enabling the model to learn high-level semantics rather than pixel-level details. This has shown significant improvements in downstream tasks that require structural understanding of the scene, such as in autonomous driving. We explored two methods of adding depth guidance to I-JEPA’s training: pixel-level guidance with ground truth metric labels,
and feature-level guidance via latent representations from a SOTA monocular depth estimation model (DepthFM).</p>

        </div>

        <div class="project-links" style="margin-top: 20px;">
            <h3>Documentation</h3>
            <ul style="list-style: none; padding: 0;">
                
                
                    <li>
                        <a href="https://github.com/Juan5713/MM_WM_AD" target="_blank" style="display: flex; align-items: center;">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="margin-right: 8px; vertical-align: middle;">
                                <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
                            </svg>
                            GitHub
                        </a>
                    </li>
                
                
                
                    <li>
                        <a href="/files/3DV_Report.pdf" target="_blank" style="display: flex; align-items: center;">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="margin-right: 8px; vertical-align: middle;">
                                <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                                <polyline points="14 2 14 8 20 8"></polyline>
                                <line x1="16" y1="13" x2="8" y2="13"></line>
                                <line x1="16" y1="17" x2="8" y2="17"></line>
                                <line x1="10" y1="9" x2="8" y2="9"></line>
                            </svg>
                            Download Project Report (PDF)
                        </a>
                    </li>
                
                
            </ul>
        </div>

        <p style="margin-top: 40px;"><a href="/projects/">← Back to Projects List</a></p>
    </div>


            
                <footer class="footer">
    
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.08b680078a3cf9c69e3dd217a5aa52cfddd4a1d850f8cdff127fdd7421a71f8b2b474be69386f67819edace92273916e7230c9054f38107db9dc6730b3530ab5.js" integrity="sha512-CLaAB4o8&#43;caePdIXpapSz93UodhQ&#43;M3/En/ddCGnH4srR0vmk4b2eBntrOkic5FucjDJBU84EH253Gcws1MKtQ=="></script>




    </body>
</html>
