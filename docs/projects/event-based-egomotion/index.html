<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="Event cameras are bio-inspired sensors that detect logarithmic changes in pixel-level illumination, generating events asynchronously with microsecond resolution. This principle provides significant advantages over conventional cameras such as high dynamic range, low latency, and low power consumption, making them ideal for dynamic scenarios and challenging lighting conditions. This work addresses the egomotion estimation of an event camera using a geometric model-based approach and optimization techniques, avoiding the use of learning methods to make it applicable in resource-constrained environments and to deeply understand the problem. The proposed methodology consists of three main blocks: normal flow estimation, inverse depth estimation, and a robust linear solver. The normal flow estimation method is based on fitting local planes on Surfaces of Active Events (SAEs), while inverse depth is tackled using optimization techniques to align events at a reference timestamp (contrast maximization approach). Finally, a robust linear solver based on the iterative RANSAC method is implemented, using the inverse depth and normal flow estimates to obtain the camera&rsquo;s linear velocity. The method&rsquo;s performance is evaluated through experiments with synthetic and real data.
" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://sergisanchezz.github.io/projects/event-based-egomotion/" />


    <title>
        
            Event-based Egomotion Estimation :: Sergi Sanchez Orvay 
        
    </title>





  <link rel="stylesheet" href="/main.min.7289daf2e88ea2c7b253a51cc5794c7cc17e7aeb7295a19d978db764a5862f25.css" integrity="sha256-cona8uiOoseyU6UcxXlMfMF&#43;eutylaGdl423ZKWGLyU=" crossorigin="anonymous">





    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Event-based Egomotion Estimation">
  <meta itemprop="description" content="Event cameras are bio-inspired sensors that detect logarithmic changes in pixel-level illumination, generating events asynchronously with microsecond resolution. This principle provides significant advantages over conventional cameras such as high dynamic range, low latency, and low power consumption, making them ideal for dynamic scenarios and challenging lighting conditions. This work addresses the egomotion estimation of an event camera using a geometric model-based approach and optimization techniques, avoiding the use of learning methods to make it applicable in resource-constrained environments and to deeply understand the problem. The proposed methodology consists of three main blocks: normal flow estimation, inverse depth estimation, and a robust linear solver. The normal flow estimation method is based on fitting local planes on Surfaces of Active Events (SAEs), while inverse depth is tackled using optimization techniques to align events at a reference timestamp (contrast maximization approach). Finally, a robust linear solver based on the iterative RANSAC method is implemented, using the inverse depth and normal flow estimates to obtain the camera’s linear velocity. The method’s performance is evaluated through experiments with synthetic and real data.">
  <meta itemprop="wordCount" content="177">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Event-based Egomotion Estimation">
  <meta name="twitter:description" content="Event cameras are bio-inspired sensors that detect logarithmic changes in pixel-level illumination, generating events asynchronously with microsecond resolution. This principle provides significant advantages over conventional cameras such as high dynamic range, low latency, and low power consumption, making them ideal for dynamic scenarios and challenging lighting conditions. This work addresses the egomotion estimation of an event camera using a geometric model-based approach and optimization techniques, avoiding the use of learning methods to make it applicable in resource-constrained environments and to deeply understand the problem. The proposed methodology consists of three main blocks: normal flow estimation, inverse depth estimation, and a robust linear solver. The normal flow estimation method is based on fitting local planes on Surfaces of Active Events (SAEs), while inverse depth is tackled using optimization techniques to align events at a reference timestamp (contrast maximization approach). Finally, a robust linear solver based on the iterative RANSAC method is implemented, using the inverse depth and normal flow estimates to obtain the camera’s linear velocity. The method’s performance is evaluated through experiments with synthetic and real data.">


















    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                Sergi Sanchez Orvay</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
                <nav class="menu">
    <ul class="menu__inner">
                <li><a href="/about/">About Me</a></li>
            
                <li><a href="/news/">News</a></li>
            
                <li><a href="/projects/">Projects</a></li>
            
                <li><a href="/publications/">Publications</a></li>
            
    </ul>
</nav>


                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
        </span>
    </span>
</header>


            
    <div class="post">
        <h1 class="post-title">Event-based Egomotion Estimation</h1>
        
        
        <div class="post-meta" style="margin-bottom: 20px;">
            Sep 2023 
            – 
            
                Jun 2024
            
        </div>
        

        

        
        <h3 style="margin-top: 25px;">Contributors</h3>
        <p>
            Authors: Sergi Sánchez Orvay
            
                <br>Supervisors: Dr. Juan Andrade-Cetto
            
        </p>
        

        <h3 style="margin-top: 30px;">Abstract</h3>
        <div class="post-content">
            <p><strong>Event cameras</strong> are bio-inspired sensors that detect logarithmic changes in pixel-level illumination, generating events asynchronously with microsecond resolution. This principle provides significant advantages over conventional cameras such as high dynamic range, low latency, and low power consumption, making them ideal for dynamic scenarios and challenging lighting conditions. This work addresses the <strong>egomotion estimation</strong> of an event camera using a geometric model-based approach and optimization techniques, avoiding the use of learning methods to make it applicable in resource-constrained environments and to deeply understand the problem. The proposed methodology consists of three main blocks: <strong>normal flow estimation</strong>, <strong>inverse depth estimation</strong>, and a <strong>robust linear solver</strong>. The normal flow estimation method is based on fitting local planes on <strong>Surfaces of Active Events (SAEs)</strong>, while inverse depth is tackled using optimization techniques to align events at a reference timestamp (contrast maximization approach). Finally, a robust linear solver based on the iterative <strong>RANSAC</strong> method is implemented, using the inverse depth and normal flow estimates to obtain the camera&rsquo;s linear velocity. The method&rsquo;s performance is evaluated through experiments with synthetic and real data.</p>

        </div>

        <div class="project-links" style="margin-top: 20px;">
            <h3>Documentation</h3>
            <ul style="list-style: none; padding: 0;">
                
                
                    <li>
                        <a href="/files/Event-based_egomotion_estimation_Sergi_Sanchez_Orvay_TFG.pdf" target="_blank">
                        Download Report (PDF)
                        </a>
                    </li>
                
                
            </ul>
        </div>

        <p style="margin-top: 40px;"><a href="/projects/">← Back to Projects List</a></p>
    </div>


            
                <footer class="footer">
    
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.08b680078a3cf9c69e3dd217a5aa52cfddd4a1d850f8cdff127fdd7421a71f8b2b474be69386f67819edace92273916e7230c9054f38107db9dc6730b3530ab5.js" integrity="sha512-CLaAB4o8&#43;caePdIXpapSz93UodhQ&#43;M3/En/ddCGnH4srR0vmk4b2eBntrOkic5FucjDJBU84EH253Gcws1MKtQ=="></script>




    </body>
</html>
